spring:
  application:
    name: dapi
  profiles:
    active: dev
  datasource:
    dynamic:
      hikari:
        connection-test-query: select 1
        idle-timeout: 600000
        pool-name: hikariPool
        max-lifetime: 1800000
        connection-timeout: 30000
        min-idle: 1
        max-pool-size: 5
        is-auto-commit: true
        connection-init-sql: select 1
        is-read-only: false
      primary: master
      strict: false
      datasource:
        master:
          url: ${mysql.url}
          username: ${mysql.username}
          password: ${mysql.password}
          continue-on-error: false
          driver-class-name: com.p6spy.engine.spy.P6SpyDriver
          p6spy: true
        second:
          url: ${mysql.second.url}
          username: ${mysql.second.username}
          password: ${mysql.second.password}
          continue-on-error: true
          driver-class-name: com.p6spy.engine.spy.P6SpyDriver
          p6spy: true
  #        pg:
  #          url: ${pg.url}
  #          username: ${pg.username}
  #          password: ${pg.password}
  #          port: 5432
  #          continue-on-error: true
  #          driver-class-name: com.p6spy.engine.spy.P6SpyDriver
  #          p6spy: true
  flyway:
    enabled: true
    encoding: UTF-8
    locations: classpath:db/migration
    sqlMigrationPrefix: V
    sqlMigrationSeparator: __
    sqlMigrationSuffixes: .sql
    validateOnMigrate: true
    baselineOnMigrate: true
    clean-disabled: true
  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 50MB
  redis:
    port: ${redis.port}
    host: ${redis.host}
    password: ${redis.password}
  rabbitmq:
    password: ${rabbitmq.password}
    port: ${rabbitmq.port}
    addresses: ${rabbitmq.address}
  kafka:
    bootstrap-servers: ${kafka.bootstrap-servers}
    properties:
      security.protocol: SASL_SSL
      sasl:
        mechanism: SCRAM-SHA-256
        jaas.config: org.apache.kafka.common.security.scram.ScramLoginModule required username="${kafka.username}" password="${kafka.password}";
    consumer:
      group-id: ${kafka.username}-consumers
      auto-offset-reset: latest
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: com.duzy
    producer:
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

mybatis-plus:
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
    auto-mapping-unknown-column-behavior: warning
    cache-enabled: false
    default-executor-type: simple
    lazy-loading-enabled: false
  global-config:
    db-config:
      logic-delete-value: 1
      logic-not-delete-value: 0
knife4j:
  enable: true
server:
  port: 8889
  tomcat:
    max-http-form-post-size: 100
debug: false
management:
  endpoints:
    web:
      exposure:
        include: '*'
  endpoint:
    health:
      show-details: always
logging:
  file:
    path: ./logs
  level:
    root: info
#    org.springframework.jdbc.core.JdbcTemplate: DEBUG


#aws配置
aws.credentials.accessKey: ${aws.credentials.accessKey}
aws.credentials.secretKey: ${aws.credentials.secretKey}
aws.region: ${aws.region}
#队列名称  fifo队列必须以 .fifo结尾
aws.sqs.endpoint: ${aws.sqs.endpoint}
queue.stand: queue1661680034119
queue.fifo: mysqs.fifo
queue.test: testQueue1661681103227
#s3
aws.s3.bucket: baohgghawss3
# ip2geo
post.ip.url: "http://ip-api.com/batch?fields=status,message,continent,continentCode,country,countryCode,region,regionName,city,district,zip,lat,lon,timezone,offset,currency,isp,org,as,asname,mobile,proxy,hosting,query&lang=zh-CN"
